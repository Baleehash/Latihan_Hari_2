{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793139f1-5286-4147-9a73-314068312e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dari CSV (IMdb Review):\n",
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "\n",
      "Dataset dari Excel (Ulasan Produk):\n",
      "                                                   Tweet     Label\n",
      "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
      "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
      "2      lokasi strategis di jalan sumatera bandung . t...  positive\n",
      "3      betapa bahagia nya diri ini saat unboxing pake...  positive\n",
      "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
      "...                                                  ...       ...\n",
      "12755  film tncfu , tidak cocok untuk penonton yang t...  negative\n",
      "12756  indihome ini mahal loh bayar nya . hanya , pen...  negative\n",
      "12757  be de gea , cowok cupu yang takut dengan pacar...  negative\n",
      "12758  valen yang sangat tidak berkualitas . konentat...  negative\n",
      "12759  restoran ini menjadi tempat pilihan saya berbu...  positive\n",
      "\n",
      "[12760 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membaca file CSV\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Membaca file Excel\n",
    "df_indo = pd.read_excel('Indonlu_Sentiment.xlsx')\n",
    "\n",
    "# Menampilkan kedua dataset\n",
    "print(\"Dataset dari CSV (IMdb Review):\")\n",
    "print(df_imdb)\n",
    "\n",
    "print(\"\\nDataset dari Excel (Ulasan Produk):\")\n",
    "print(df_indo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92807907-348e-4729-ac60-025dcbbc13e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Risma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset IMDB setelah pembersihan:\n",
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  one reviewers mentioned watching oz episode yo...  \n",
      "1  wonderful little production br br filming tech...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically theres family little boy jake thinks...  \n",
      "4  petter matteis love time money visually stunni...  \n",
      "\n",
      "Dataset Bahasa Indonesia setelah pembersihan:\n",
      "                                               Tweet  \\\n",
      "0  warung ini dimiliki oleh pengusaha pabrik tahu...   \n",
      "1  mohon ulama lurus dan k212 mmbri hujjah partai...   \n",
      "2  lokasi strategis di jalan sumatera bandung . t...   \n",
      "3  betapa bahagia nya diri ini saat unboxing pake...   \n",
      "4  duh . jadi mahasiswa jangan sombong dong . kas...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  warung dimiliki pengusaha pabrik puluhan terke...  \n",
      "1  mohon ulama lurus mmbri hujjah partai diwlh su...  \n",
      "2  lokasi strategis jalan sumatera bandung nya ny...  \n",
      "3  betapa bahagia nya unboxing paket barang nya b...  \n",
      "4  duh mahasiswa sombong kasih kartu kuning belaj...  \n"
     ]
    }
   ],
   "source": [
    "# Import untuk preprocessing\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Fungsi membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Konversi teks menjadi lowercase\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Menghapus teks dalam tanda []\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Menghapus angka\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Menghapus tanda baca\n",
    "    text = re.sub(r'\\n', '', text)  # Menghapus newline\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Menghapus spasi ganda\n",
    "    return text\n",
    "\n",
    "# Fungsi menghapus stopwords\n",
    "def remove_stopwords(text, language='english'):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Load dataset\n",
    "df_imdb = pd.read_csv('IMDB Dataset.csv')\n",
    "df_indo = pd.read_excel('Indonlu_Sentiment.xlsx')\n",
    "\n",
    "# Preprocessing dataset IMDB\n",
    "df_imdb['cleaned_text'] = df_imdb['review'].apply(clean_text)\n",
    "df_imdb['cleaned_text'] = df_imdb['cleaned_text'].apply(remove_stopwords, language='english')\n",
    "\n",
    "# Preprocessing dataset bahasa Indonesia\n",
    "df_indo['cleaned_text'] = df_indo['Tweet'].apply(clean_text)\n",
    "df_indo['cleaned_text'] = df_indo['cleaned_text'].apply(remove_stopwords, language='indonesian')\n",
    "\n",
    "# Menampilkan hasil pembersihan\n",
    "print(\"Dataset IMDB setelah pembersihan:\")\n",
    "print(df_imdb[['review', 'cleaned_text']].head())\n",
    "\n",
    "print(\"\\nDataset Bahasa Indonesia setelah pembersihan:\")\n",
    "print(df_indo[['Tweet', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "006af4f0-d3a9-49ca-8ed8-27f0e871390e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Vektorisasi dataset IMDB\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer_imdb \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Membuat instance baru untuk dataset IMDB\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_imdb \u001b[38;5;241m=\u001b[39m vectorizer_imdb\u001b[38;5;241m.\u001b[39mfit_transform(df_imdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Vektorisasi\u001b[39;00m\n\u001b[0;32m      7\u001b[0m terms_imdb \u001b[38;5;241m=\u001b[39m vectorizer_imdb\u001b[38;5;241m.\u001b[39mget_feature_names_out()  \u001b[38;5;66;03m# Mendapatkan nama fitur\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Menampilkan beberapa hasil vektorisasi IMDB\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1287\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1284\u001b[0m         \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m j_indices\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1288\u001b[0m values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m   1289\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import untuk vektorisasi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vektorisasi dataset IMDB\n",
    "vectorizer_imdb = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=5)  # Membuat instance baru untuk dataset IMDB\n",
    "X_imdb = vectorizer_imdb.fit_transform(df_imdb['cleaned_text'])  # Vektorisasi\n",
    "terms_imdb = vectorizer_imdb.get_feature_names_out()  # Mendapatkan nama fitur\n",
    "\n",
    "# Menampilkan beberapa hasil vektorisasi IMDB\n",
    "print(\"Vektorisasi IMDB (beberapa baris):\")\n",
    "print(X_imdb[0:5, :].toarray())  # Menampilkan 5 baris pertama\n",
    "print(\"Terms IMDB:\")\n",
    "print(terms_imdb[:10])  # Menampilkan 10 istilah pertama\n",
    "\n",
    "# Vektorisasi dataset Bahasa Indonesia\n",
    "vectorizer_indo = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=5)  # Membuat instance baru untuk dataset Bahasa Indonesia\n",
    "X_indo = vectorizer_indo.fit_transform(df_indo['cleaned_text'])  # Vektorisasi\n",
    "terms_indo = vectorizer_indo.get_feature_names_out()  # Mendapatkan nama fitur\n",
    "\n",
    "# Menampilkan beberapa hasil vektorisasi Bahasa Indonesia\n",
    "print(\"\\nVektorisasi Bahasa Indonesia (beberapa baris):\")\n",
    "print(X_indo[0:5, :].toarray())  # Menampilkan 5 baris pertama\n",
    "print(\"Terms Bahasa Indonesia:\")\n",
    "print(terms_indo[:10])  # Menampilkan 10 istilah pertama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7692ec9f-bde0-49a1-a7e9-7c1750982cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape IMDB: (40000, 4000), Test Shape IMDB: (10000, 4000)\n",
      "Train Shape Indonesian: (10208, 4000), Test Shape Indonesian: (2552, 4000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Membagi dataset menjadi training dan testing (80% training, 20% testing)\n",
    "X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb = train_test_split(X_imdb, y_imdb, test_size=0.2, random_state=42)\n",
    "X_train_indo, X_test_indo, y_train_indo, y_test_indo = train_test_split(X_indo, y_indo, test_size=0.2, random_state=42)\n",
    "\n",
    "# Menampilkan bentuk data latih dan data uji untuk dataset IMDB\n",
    "print(f'Train Shape IMDB: {X_train_imdb.shape}, Test Shape IMDB: {X_test_imdb.shape}')\n",
    "\n",
    "# Menampilkan bentuk data latih dan data uji untuk dataset Bahasa Indonesia\n",
    "print(f'Train Shape Indonesian: {X_train_indo.shape}, Test Shape Indonesian: {X_test_indo.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119796ae-74a6-4697-a5dc-6f31b63891e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.18 GiB for an array with shape (39743, 4001) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Melatih model dengan data latih IMDB\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(X_train_imdb, y_train_imdb)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Memprediksi data uji IMDB\u001b[39;00m\n\u001b[0;32m     11\u001b[0m y_pred_lr_imdb \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test_imdb)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[0;32m   1297\u001b[0m     path_func(\n\u001b[0;32m   1298\u001b[0m         X,\n\u001b[0;32m   1299\u001b[0m         y,\n\u001b[0;32m   1300\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[0;32m   1301\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[0;32m   1302\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[0;32m   1303\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1304\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1305\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1306\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[0;32m   1307\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[0;32m   1308\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1309\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1310\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1311\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1312\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[0;32m   1313\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[0;32m   1314\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[0;32m   1315\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1316\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m   1317\u001b[0m     )\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[0;32m   1319\u001b[0m )\n\u001b[0;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:352\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m Y_multi\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    350\u001b[0m             Y_multi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y_multi, Y_multi])\n\u001b[1;32m--> 352\u001b[0m     w0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m    353\u001b[0m         (classes\u001b[38;5;241m.\u001b[39msize, n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(fit_intercept)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    354\u001b[0m     )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# IMPORTANT NOTE:\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# All solvers relying on LinearModelLoss need to scale the penalty with n_samples\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# or the sum of sample weights because the implemented logistic regression\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# instead of (as LinearModelLoss does)\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m#     mean(pointwise_loss) + 1/C * penalty\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnewton-cholesky\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# This needs to be calculated after sample_weight is multiplied by\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# class_weight. It is even tested that passing class_weight is equivalent to\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# passing sample_weights according to class_weight.\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.18 GiB for an array with shape (39743, 4001) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Inisialisasi model Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Melatih model dengan data latih IMDB\n",
    "lr_model.fit(X_train_imdb, y_train_imdb)\n",
    "\n",
    "# Memprediksi data uji IMDB\n",
    "y_pred_lr_imdb = lr_model.predict(X_test_imdb)\n",
    "\n",
    "# Menghitung akurasi IMDB\n",
    "print(\"Akurasi Logistic Regression (IMDB):\", accuracy_score(y_test_imdb, y_pred_lr_imdb))\n",
    "print(classification_report(y_test_imdb, y_pred_lr_imdb))\n",
    "\n",
    "# Melatih model dengan data latih Indo\n",
    "lr_model.fit(X_train_indo, y_train_indo)\n",
    "\n",
    "# Memprediksi data uji Indo\n",
    "y_pred_lr_indo = lr_model.predict(X_test_indo)\n",
    "\n",
    "# Menghitung akurasi Indo\n",
    "print(\"Akurasi Logistic Regression (Indo):\", accuracy_score(y_test_indo, y_pred_lr_indo))\n",
    "print(classification_report(y_test_indo, y_pred_lr_indo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
